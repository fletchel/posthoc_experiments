{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code to replicate the experiments mentioned in the Google doc.\n",
    "\n",
    "The graphs themselves were made by 4o from raw text outputs, so there's no code here for making the actual visualisations.\n",
    "\n",
    "Warning: A lot of this code is ugly and inefficient. Time constraints! If I were doing a larger scale project I would definitely spend some time making the code nicer.\n",
    "\n",
    "Big thanks to my bestie o1 who helped with a lot of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load in the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa863fe63864dac85b9adb55829a221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from nnsight import NNsight\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "small_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "small_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "small_model = NNsight(small_model)\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "large_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "large_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "large_model = NNsight(large_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load in the generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/home/lfletcher/MATS_app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/generations/high_school_gens_large.pkl', 'rb') as f:\n",
    "\n",
    "    large_gens = pickle.load(f)\n",
    "\n",
    "with open('data/generations/high_school_gens_small.pkl', 'rb') as f:\n",
    "\n",
    "    small_gens = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the generations to only consider those <2000 tokens and which follow the format of having an answer in \\boxed{}.\n",
    "\n",
    "This code also adds the predicted_label and token position of the predicted label to the generation dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "small_tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "\n",
    "small_filtered_gens = []\n",
    "for item in small_gens:\n",
    "    full_text = item['full_generated_text']\n",
    "    prompt = item['prompt']\n",
    "    # Estimate tokenized length by splitting on whitespace and punctuation\n",
    "    tokenized_length = len(small_tokenizer.tokenize(full_text)) - len(small_tokenizer.tokenize(prompt))\n",
    "\n",
    "    # Check tokenized length condition\n",
    "    if tokenized_length < 2000:\n",
    "        # Extract boxed answer using regex patterns\n",
    "        match = re.search(r'\\\\boxed{\\(?([A-Da-d])\\)?}', full_text)\n",
    "        if match:\n",
    "            label = match.group(1).upper()  # Normalize to uppercase\n",
    "            label_index = ord(label) - ord('A')  # Convert A-D to 0-3 index\n",
    "            answer_str = match.group(0)  # Full match (e.g., \\boxed{C})\n",
    "            char_index = full_text.find(answer_str)  # Character index\n",
    "            token_index = len(small_tokenizer.tokenize(full_text[:char_index]))  # Convert to token index\n",
    "\n",
    "            item['predicted_label'] = label_index\n",
    "            item['answer_token_index'] = token_index  # Store token index of answer\n",
    "            small_filtered_gens.append(item)\n",
    "\n",
    "large_filtered_gens = []\n",
    "for item in large_gens:\n",
    "    full_text = item['full_generated_text']\n",
    "    prompt = item['prompt']\n",
    "    # Estimate tokenized length by splitting on whitespace and punctuation\n",
    "    tokenized_length = len(small_tokenizer.tokenize(full_text)) - len(small_tokenizer.tokenize(prompt))\n",
    "\n",
    "    # Check tokenized length condition\n",
    "    if tokenized_length < 2000:\n",
    "        # Extract boxed answer using regex patterns\n",
    "        match = re.search(r'\\\\boxed{\\(?([A-Da-d])\\)?}', full_text)\n",
    "        if match:\n",
    "            label = match.group(1).upper()  # Normalize to uppercase\n",
    "            label_index = ord(label) - ord('A')  # Convert A-D to 0-3 index\n",
    "            answer_str = match.group(0)  # Full match (e.g., \\boxed{C})\n",
    "            char_index = full_text.find(answer_str)  # Character index\n",
    "            token_index = len(large_tokenizer.tokenize(full_text[:char_index]))  # Convert to token index\n",
    "\n",
    "            item['predicted_label'] = label_index\n",
    "            item['answer_token_index'] = token_index  # Store token index of answer\n",
    "            large_filtered_gens.append(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we find the incorrect generations for each of the small and large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624\n",
      "421\n"
     ]
    }
   ],
   "source": [
    "small_incorrect_gens = [gen for gen in small_filtered_gens if gen['predicted_label'] != gen['correct_label']]\n",
    "large_incorrect_gens = [gen for gen in large_filtered_gens if gen['predicted_label'] != gen['correct_label']]\n",
    "                           \n",
    "print(len(small_incorrect_gens))\n",
    "print(len(large_incorrect_gens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the pre and post CoT logits for the predicted_answer for the small and large model\n",
    "\n",
    "This code is monstrously inefficient and not batched. It takes like 10 mins to run on an A100. Sorry!\n",
    "\n",
    "The resulting lists are dictionaries of the form\n",
    "\n",
    "    {\n",
    "        \"final_answer\": the final answer given by the model after the CoT,\n",
    "        \"post_logits\": the logits of A-D after the CoT,\n",
    "        \"pre_logits\": the logits of A-D before the CoT,\n",
    "        \"subject\": the subject of this datapoint (i.e. geography, math etc.)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting logits for small model\n",
      "Getting logits for generation 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting logits for generation 10\n",
      "Getting logits for generation 20\n",
      "Getting logits for generation 30\n",
      "Getting logits for generation 40\n",
      "Getting logits for generation 50\n",
      "Getting logits for generation 60\n",
      "Getting logits for generation 70\n",
      "Getting logits for generation 80\n",
      "Getting logits for generation 90\n",
      "Getting logits for generation 100\n",
      "Getting logits for generation 110\n",
      "Getting logits for generation 120\n",
      "Getting logits for generation 130\n",
      "Getting logits for generation 140\n",
      "Getting logits for generation 150\n",
      "Getting logits for generation 160\n",
      "Getting logits for generation 170\n",
      "Getting logits for generation 180\n",
      "Getting logits for generation 190\n",
      "Getting logits for generation 200\n",
      "Getting logits for generation 210\n",
      "Getting logits for generation 220\n",
      "Getting logits for generation 230\n",
      "Getting logits for generation 240\n",
      "Getting logits for generation 250\n",
      "Getting logits for generation 260\n",
      "Getting logits for generation 270\n",
      "Getting logits for generation 280\n",
      "Getting logits for generation 290\n",
      "Getting logits for generation 300\n",
      "Getting logits for generation 310\n",
      "Getting logits for generation 320\n",
      "Getting logits for generation 330\n",
      "Getting logits for generation 340\n",
      "Getting logits for generation 350\n",
      "Getting logits for generation 360\n",
      "Getting logits for generation 370\n",
      "Getting logits for generation 380\n",
      "Getting logits for generation 390\n",
      "Getting logits for generation 400\n",
      "Getting logits for generation 410\n",
      "Getting logits for generation 420\n",
      "Getting logits for generation 430\n",
      "Getting logits for generation 440\n",
      "Getting logits for generation 450\n",
      "Getting logits for generation 460\n",
      "Getting logits for generation 470\n",
      "Getting logits for generation 480\n",
      "Getting logits for generation 490\n",
      "Getting logits for generation 500\n",
      "Getting logits for generation 510\n",
      "Getting logits for generation 520\n",
      "Getting logits for generation 530\n",
      "Getting logits for generation 540\n",
      "Getting logits for generation 550\n",
      "Getting logits for generation 560\n",
      "Getting logits for generation 570\n",
      "Getting logits for generation 580\n",
      "Getting logits for generation 590\n",
      "Getting logits for generation 600\n",
      "Getting logits for generation 610\n",
      "Getting logits for generation 620\n",
      "\n",
      "Getting logits for large model\n",
      "Getting logits for generation 0\n",
      "Getting logits for generation 10\n",
      "Getting logits for generation 20\n",
      "Getting logits for generation 30\n",
      "Getting logits for generation 40\n",
      "Getting logits for generation 50\n",
      "Getting logits for generation 60\n",
      "Getting logits for generation 70\n",
      "Getting logits for generation 80\n",
      "Getting logits for generation 90\n",
      "Getting logits for generation 100\n",
      "Getting logits for generation 110\n",
      "Getting logits for generation 120\n",
      "Getting logits for generation 130\n",
      "Getting logits for generation 140\n",
      "Getting logits for generation 150\n",
      "Getting logits for generation 160\n",
      "Getting logits for generation 170\n",
      "Getting logits for generation 180\n",
      "Getting logits for generation 190\n",
      "Getting logits for generation 200\n",
      "Getting logits for generation 210\n",
      "Getting logits for generation 220\n",
      "Getting logits for generation 230\n",
      "Getting logits for generation 240\n",
      "Getting logits for generation 250\n",
      "Getting logits for generation 260\n",
      "Getting logits for generation 270\n",
      "Getting logits for generation 280\n",
      "Getting logits for generation 290\n",
      "Getting logits for generation 300\n",
      "Getting logits for generation 310\n",
      "Getting logits for generation 320\n",
      "Getting logits for generation 330\n",
      "Getting logits for generation 340\n",
      "Getting logits for generation 350\n",
      "Getting logits for generation 360\n",
      "Getting logits for generation 370\n",
      "Getting logits for generation 380\n",
      "Getting logits for generation 390\n",
      "Getting logits for generation 400\n",
      "Getting logits for generation 410\n",
      "Getting logits for generation 420\n"
     ]
    }
   ],
   "source": [
    "from utils import get_pre_post_cot_logits\n",
    "\n",
    "small_logits = []\n",
    "large_logits = []\n",
    "\n",
    "print(\"Getting logits for small model\")\n",
    "for i, gen in enumerate(small_incorrect_gens):\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Getting logits for generation {i}\")\n",
    "\n",
    "    small_logits.append(get_pre_post_cot_logits(gen, small_model, small_tokenizer))\n",
    "\n",
    "print(\"\\nGetting logits for large model\")\n",
    "for i, gen in enumerate(large_incorrect_gens):\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Getting logits for generation {i}\")\n",
    "        \n",
    "    large_logits.append(get_pre_post_cot_logits(gen, large_model, large_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have these logits, we get a couple of relevant quantities\n",
    "\n",
    "1) the probability of the final answer pre and post CoT (saved in cur_prob_dict)\n",
    "2) the probability of all 4 of A-D pre and post CoT (saved in small/large_probs_pre_all and small/large_probs_post_all)\n",
    "\n",
    "The latter we calculate to check some assumptions that we're relying on (see Google doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2373827/2422553225.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  cur_prob_prompt = F.softmax(logits_prompt)\n",
      "/tmp/ipykernel_2373827/2422553225.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  cur_prob_cot = F.softmax(logits_cot)\n",
      "/tmp/ipykernel_2373827/2422553225.py:39: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  cur_prob_prompt = F.softmax(logits_prompt)\n",
      "/tmp/ipykernel_2373827/2422553225.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  cur_prob_cot = F.softmax(logits_cot)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "small_probs = []\n",
    "\n",
    "small_probs_pre_all = []\n",
    "small_probs_post_all = []\n",
    "\n",
    "for i in range(len(small_incorrect_gens)):\n",
    "\n",
    "    cur_gen = small_logits[i]\n",
    "    predict_idx = small_incorrect_gens[i]['predicted_label']\n",
    "\n",
    "    logits_prompt = cur_gen['pre_logits']\n",
    "    logits_cot = cur_gen['post_logits']\n",
    "    \n",
    "    cur_prob_prompt = F.softmax(logits_prompt)\n",
    "    cur_prob_cot = F.softmax(logits_cot)\n",
    "\n",
    "    small_probs_pre_all.append(cur_prob_prompt.tolist())\n",
    "    small_probs_post_all.append(cur_prob_cot.tolist())\n",
    "\n",
    "    cur_prob_dict = {\"pre_prob\":cur_prob_prompt[predict_idx], \"post_prob\":cur_prob_cot[predict_idx], \"subject\":cur_gen[\"subject\"]}\n",
    "    small_probs.append(cur_prob_dict)\n",
    "\n",
    "\n",
    "large_probs = []\n",
    "\n",
    "large_probs_pre_all = []\n",
    "large_probs_post_all = []\n",
    "\n",
    "for i in range(len(large_incorrect_gens)):\n",
    "\n",
    "    cur_gen = large_logits[i]\n",
    "    predict_idx = large_incorrect_gens[i]['predicted_label']\n",
    "\n",
    "    logits_prompt = cur_gen['pre_logits']\n",
    "    logits_cot = cur_gen['post_logits']\n",
    "    \n",
    "    cur_prob_prompt = F.softmax(logits_prompt)\n",
    "    cur_prob_cot = F.softmax(logits_cot)\n",
    "\n",
    "    large_probs_pre_all.append(cur_prob_prompt.tolist())\n",
    "    large_probs_post_all.append(cur_prob_cot.tolist())\n",
    "\n",
    "    cur_prob_dict = {\"pre_prob\":cur_prob_prompt[predict_idx], \"post_prob\":cur_prob_cot[predict_idx], \"subject\":cur_gen[\"subject\"]}\n",
    "    large_probs.append(cur_prob_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the average probability of the final (post-CoT) answer pre-CoT for each model. This is the main result of this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for small model\n",
      "Subject: high_school_biology, Mean Pre-CoT Probability: 0.9994, Count: 81\n",
      "Subject: high_school_chemistry, Mean Pre-CoT Probability: 0.9687, Count: 28\n",
      "Subject: high_school_computer_science, Mean Pre-CoT Probability: 0.9997, Count: 11\n",
      "Subject: high_school_european_history, Mean Pre-CoT Probability: 0.9809, Count: 16\n",
      "Subject: high_school_geography, Mean Pre-CoT Probability: 0.9956, Count: 64\n",
      "Subject: high_school_government_and_politics, Mean Pre-CoT Probability: 0.9995, Count: 66\n",
      "Subject: high_school_macroeconomics, Mean Pre-CoT Probability: 0.9969, Count: 74\n",
      "Subject: high_school_mathematics, Mean Pre-CoT Probability: 0.8658, Count: 3\n",
      "Subject: high_school_microeconomics, Mean Pre-CoT Probability: 0.9985, Count: 50\n",
      "Subject: high_school_physics, Mean Pre-CoT Probability: 0.9986, Count: 14\n",
      "Subject: high_school_psychology, Mean Pre-CoT Probability: 0.9888, Count: 146\n",
      "Subject: high_school_statistics, Mean Pre-CoT Probability: 0.9990, Count: 17\n",
      "Subject: high_school_us_history, Mean Pre-CoT Probability: 0.9985, Count: 30\n",
      "Subject: high_school_world_history, Mean Pre-CoT Probability: 0.9996, Count: 24\n",
      "\n",
      "Results for large model\n",
      "Subject: high_school_biology, Mean Pre-CoT Probability: 0.9993, Count: 128\n",
      "Subject: high_school_chemistry, Mean Pre-CoT Probability: 0.9784, Count: 41\n",
      "Subject: high_school_computer_science, Mean Pre-CoT Probability: 0.9990, Count: 16\n",
      "Subject: high_school_european_history, Mean Pre-CoT Probability: 0.9903, Count: 32\n",
      "Subject: high_school_geography, Mean Pre-CoT Probability: 0.9973, Count: 119\n",
      "Subject: high_school_government_and_politics, Mean Pre-CoT Probability: 0.9994, Count: 113\n",
      "Subject: high_school_macroeconomics, Mean Pre-CoT Probability: 0.9970, Count: 126\n",
      "Subject: high_school_mathematics, Mean Pre-CoT Probability: 0.8658, Count: 3\n",
      "Subject: high_school_microeconomics, Mean Pre-CoT Probability: 0.9982, Count: 60\n",
      "Subject: high_school_physics, Mean Pre-CoT Probability: 0.9949, Count: 24\n",
      "Subject: high_school_psychology, Mean Pre-CoT Probability: 0.9925, Count: 249\n",
      "Subject: high_school_statistics, Mean Pre-CoT Probability: 0.9992, Count: 28\n",
      "Subject: high_school_us_history, Mean Pre-CoT Probability: 0.9980, Count: 54\n",
      "Subject: high_school_world_history, Mean Pre-CoT Probability: 0.9968, Count: 52\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "subject_stats = defaultdict(lambda: {\"sum\": 0.0, \"count\": 0})\n",
    "\n",
    "# Accumulate sums and counts\n",
    "for entry in small_probs:\n",
    "    subj = entry[\"subject\"]\n",
    "    subject_stats[subj][\"sum\"] += entry[\"pre_prob\"]\n",
    "    subject_stats[subj][\"count\"] += 1\n",
    "\n",
    "# Compute mean and store results\n",
    "result = []\n",
    "for subj, stats in subject_stats.items():\n",
    "    mean_prob = stats[\"sum\"] / stats[\"count\"]\n",
    "    result.append({\n",
    "        \"subject\": subj,\n",
    "        \"mean_pre_prob\": mean_prob,\n",
    "        \"count\": stats[\"count\"]\n",
    "    })\n",
    "\n",
    "print(\"Results for small model\")\n",
    "for entry in result:\n",
    "    print(f\"Subject: {entry['subject']}, Mean Pre-CoT Probability: {entry['mean_pre_prob']:.4f}, Count: {entry['count']}\")\n",
    "\n",
    "\n",
    "# Accumulate sums and counts\n",
    "for entry in large_probs:\n",
    "    subj = entry[\"subject\"]\n",
    "    subject_stats[subj][\"sum\"] += entry[\"pre_prob\"]\n",
    "    subject_stats[subj][\"count\"] += 1\n",
    "\n",
    "# Compute mean and store results\n",
    "result = []\n",
    "for subj, stats in subject_stats.items():\n",
    "    mean_prob = stats[\"sum\"] / stats[\"count\"]\n",
    "    result.append({\n",
    "        \"subject\": subj,\n",
    "        \"mean_pre_prob\": mean_prob,\n",
    "        \"count\": stats[\"count\"]\n",
    "    })\n",
    "\n",
    "print(\"\\nResults for large model\")\n",
    "for entry in result:\n",
    "    print(f\"Subject: {entry['subject']}, Mean Pre-CoT Probability: {entry['mean_pre_prob']:.4f}, Count: {entry['count']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we check for one possible alternative explanation for these results, namely that both the pre and post probabilities are biased in the same way \n",
    "\n",
    "(i.e., if the model says B 50% of the time both pre and post CoT, then our metric of mean pre-CoT probability will be >50% without post-hoc reasoning being necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.28692206 0.24727051 0.24898808 0.21681934]\n",
      "[0.22922276 0.27841208 0.24832366 0.24404151]\n",
      "[0.30003469 0.22485375 0.26373966 0.2113719 ]\n",
      "[0.25316039 0.35668414 0.21492729 0.17522819]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "small_probs_prompt_all = np.array(small_probs_pre_all)\n",
    "small_probs_cot_all = np.array(small_probs_post_all)\n",
    "\n",
    "large_probs_prompt_all = np.array(large_probs_pre_all)\n",
    "large_probs_cot_all = np.array(large_probs_post_all)\n",
    "\n",
    "print(np.mean(large_probs_prompt_all, axis=0))\n",
    "print(np.mean(large_probs_cot_all, axis=0))\n",
    "\n",
    "print(np.mean(small_probs_prompt_all, axis=0))\n",
    "print(np.mean(small_probs_cot_all, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two distributions are not biased in the same way (although the small model in particular has got a weird bias for B pre-CoT and A post-CoT)\n",
    "\n",
    "In particular, if pre/post were independent, then for both large and small models the probability of ans_pre = ans_post is approximately 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've moved the code which actually extracts the features to the appendix and just load the features here. This is because the code extracting the features is pretty slow (particularly for the big model). Feel free to skip to the appendix to take a look at that if you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we load the features here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/probe_features/small_features.pkl', 'rb') as f:\n",
    "\n",
    "    small_features = pickle.load(f)\n",
    "\n",
    "with open('data/probe_features/small_labels.pkl', 'rb') as f:\n",
    "\n",
    "    small_labels = pickle.load(f)\n",
    "\n",
    "with open('data/probe_features/large_features.pkl', 'rb') as f:\n",
    "\n",
    "    large_features = pickle.load(f)\n",
    "\n",
    "with open('data/probe_features/large_labels.pkl', 'rb') as f:\n",
    "\n",
    "    large_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split into test and train sets, along with randomly discarding datapoints until we have balanced labels [balance_classes()] (this is basically the most naive thing you could possibly do but time constraints!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import balance_classes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepare_splits(features, labels):\n",
    "\n",
    "    bal_features, bal_labels = balance_classes(features, labels)\n",
    "\n",
    "    features_np = features.numpy()\n",
    "    labels_np = labels.numpy()\n",
    "\n",
    "    # Perform an 80â€“20 split\n",
    "    train_features_np, test_features_np, \\\n",
    "    train_labels_np, test_labels_np = train_test_split(\n",
    "        features_np,\n",
    "        labels_np,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=labels_np  # Stratify for class balance\n",
    "    )\n",
    "\n",
    "    # Convert NumPy arrays back to tensors\n",
    "    train_features = torch.tensor(train_features_np)\n",
    "    test_features  = torch.tensor(test_features_np)\n",
    "    train_labels    = torch.tensor(train_labels_np)\n",
    "    test_labels     = torch.tensor(test_labels_np)\n",
    "\n",
    "    return train_features, test_features, train_labels, test_labels\n",
    "\n",
    "small_train_features, small_test_features, small_train_labels, small_test_labels = prepare_splits(small_features, small_labels)\n",
    "large_train_features, large_test_features, large_train_labels, large_test_labels = prepare_splits(large_features, large_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the linear probe 3 times at each position/layer for the small model and print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0, Pos 0 -> Mean: 0.2579, Std: 0.0304\n",
      "Layer 0, Pos 1 -> Mean: 0.2350, Std: 0.0122\n",
      "Layer 0, Pos 2 -> Mean: 0.2512, Std: 0.0059\n",
      "Layer 0, Pos 3 -> Mean: 0.2808, Std: 0.0041\n",
      "Layer 0, Pos 4 -> Mean: 0.2407, Std: 0.0257\n",
      "Layer 0, Pos 5 -> Mean: 0.2770, Std: 0.0239\n",
      "Layer 0, Pos 6 -> Mean: 0.2923, Std: 0.0084\n",
      "Layer 0, Pos 7 -> Mean: 0.2693, Std: 0.0041\n",
      "Layer 0, Pos 8 -> Mean: 0.4499, Std: 0.0062\n",
      "Layer 0, Pos 9 -> Mean: 0.4604, Std: 0.0179\n",
      "Layer 0, Pos 10 -> Mean: 0.4308, Std: 0.0105\n",
      "Layer 0, Pos 11 -> Mean: 0.4460, Std: 0.0172\n",
      "Layer 0, Pos 12 -> Mean: 0.5062, Std: 0.0236\n",
      "Layer 0, Pos 13 -> Mean: 0.2359, Std: 0.0049\n",
      "Layer 0, Pos 14 -> Mean: 0.2369, Std: 0.0049\n",
      "Layer 0, Pos 15 -> Mean: 0.2521, Std: 0.0047\n",
      "Layer 0, Pos 16 -> Mean: 0.2063, Std: 0.0047\n",
      "Layer 1, Pos 0 -> Mean: 0.2225, Std: 0.0027\n",
      "Layer 1, Pos 1 -> Mean: 0.2197, Std: 0.0111\n",
      "Layer 1, Pos 2 -> Mean: 0.2598, Std: 0.0164\n",
      "Layer 1, Pos 3 -> Mean: 0.2760, Std: 0.0199\n",
      "Layer 1, Pos 4 -> Mean: 0.2598, Std: 0.0082\n",
      "Layer 1, Pos 5 -> Mean: 0.2798, Std: 0.0129\n",
      "Layer 1, Pos 6 -> Mean: 0.2770, Std: 0.0231\n",
      "Layer 1, Pos 7 -> Mean: 0.2588, Std: 0.0071\n",
      "Layer 1, Pos 8 -> Mean: 0.4909, Std: 0.0118\n",
      "Layer 1, Pos 9 -> Mean: 0.5626, Std: 0.0027\n",
      "Layer 1, Pos 10 -> Mean: 0.5712, Std: 0.0059\n",
      "Layer 1, Pos 11 -> Mean: 0.5501, Std: 0.0130\n",
      "Layer 1, Pos 12 -> Mean: 0.6323, Std: 0.0202\n",
      "Layer 1, Pos 13 -> Mean: 0.2875, Std: 0.0054\n",
      "Layer 1, Pos 14 -> Mean: 0.2598, Std: 0.0049\n",
      "Layer 1, Pos 15 -> Mean: 0.2588, Std: 0.0068\n",
      "Layer 1, Pos 16 -> Mean: 0.2369, Std: 0.0036\n",
      "Layer 2, Pos 0 -> Mean: 0.2455, Std: 0.0351\n",
      "Layer 2, Pos 1 -> Mean: 0.2407, Std: 0.0200\n",
      "Layer 2, Pos 2 -> Mean: 0.2464, Std: 0.0266\n",
      "Layer 2, Pos 3 -> Mean: 0.2741, Std: 0.0311\n",
      "Layer 2, Pos 4 -> Mean: 0.2751, Std: 0.0244\n",
      "Layer 2, Pos 5 -> Mean: 0.2655, Std: 0.0179\n",
      "Layer 2, Pos 6 -> Mean: 0.2770, Std: 0.0049\n",
      "Layer 2, Pos 7 -> Mean: 0.2798, Std: 0.0250\n",
      "Layer 2, Pos 8 -> Mean: 0.6094, Std: 0.0237\n",
      "Layer 2, Pos 9 -> Mean: 0.7144, Std: 0.0097\n",
      "Layer 2, Pos 10 -> Mean: 0.8309, Std: 0.0102\n",
      "Layer 2, Pos 11 -> Mean: 0.6533, Std: 0.0041\n",
      "Layer 2, Pos 12 -> Mean: 0.8472, Std: 0.0089\n",
      "Layer 2, Pos 13 -> Mean: 0.2646, Std: 0.0075\n",
      "Layer 2, Pos 14 -> Mean: 0.2655, Std: 0.0059\n",
      "Layer 2, Pos 15 -> Mean: 0.2693, Std: 0.0047\n",
      "Layer 2, Pos 16 -> Mean: 0.2856, Std: 0.0054\n",
      "Layer 3, Pos 0 -> Mean: 0.2369, Std: 0.0149\n",
      "Layer 3, Pos 1 -> Mean: 0.2665, Std: 0.0191\n",
      "Layer 3, Pos 2 -> Mean: 0.2512, Std: 0.0115\n",
      "Layer 3, Pos 3 -> Mean: 0.2741, Std: 0.0189\n",
      "Layer 3, Pos 4 -> Mean: 0.2770, Std: 0.0225\n",
      "Layer 3, Pos 5 -> Mean: 0.2493, Std: 0.0124\n",
      "Layer 3, Pos 6 -> Mean: 0.2894, Std: 0.0130\n",
      "Layer 3, Pos 7 -> Mean: 0.2674, Std: 0.0177\n",
      "Layer 3, Pos 8 -> Mean: 0.7202, Std: 0.0129\n",
      "Layer 3, Pos 9 -> Mean: 0.8415, Std: 0.0036\n",
      "Layer 3, Pos 10 -> Mean: 0.9274, Std: 0.0059\n",
      "Layer 3, Pos 11 -> Mean: 0.9207, Std: 0.0071\n",
      "Layer 3, Pos 12 -> Mean: 0.9675, Std: 0.0036\n",
      "Layer 3, Pos 13 -> Mean: 0.2846, Std: 0.0075\n",
      "Layer 3, Pos 14 -> Mean: 0.2875, Std: 0.0075\n",
      "Layer 3, Pos 15 -> Mean: 0.2388, Std: 0.0027\n",
      "Layer 3, Pos 16 -> Mean: 0.2455, Std: 0.0054\n",
      "Layer 4, Pos 0 -> Mean: 0.2330, Std: 0.0176\n",
      "Layer 4, Pos 1 -> Mean: 0.2378, Std: 0.0169\n",
      "Layer 4, Pos 2 -> Mean: 0.2340, Std: 0.0014\n",
      "Layer 4, Pos 3 -> Mean: 0.3104, Std: 0.0221\n",
      "Layer 4, Pos 4 -> Mean: 0.2674, Std: 0.0105\n",
      "Layer 4, Pos 5 -> Mean: 0.2512, Std: 0.0191\n",
      "Layer 4, Pos 6 -> Mean: 0.2598, Std: 0.0338\n",
      "Layer 4, Pos 7 -> Mean: 0.2627, Std: 0.0143\n",
      "Layer 4, Pos 8 -> Mean: 0.6877, Std: 0.0230\n",
      "Layer 4, Pos 9 -> Mean: 0.8128, Std: 0.0036\n",
      "Layer 4, Pos 10 -> Mean: 0.9341, Std: 0.0102\n",
      "Layer 4, Pos 11 -> Mean: 0.9370, Std: 0.0023\n",
      "Layer 4, Pos 12 -> Mean: 0.9675, Std: 0.0014\n",
      "Layer 4, Pos 13 -> Mean: 0.2531, Std: 0.0137\n",
      "Layer 4, Pos 14 -> Mean: 0.2875, Std: 0.0143\n",
      "Layer 4, Pos 15 -> Mean: 0.2760, Std: 0.0071\n",
      "Layer 4, Pos 16 -> Mean: 0.2751, Std: 0.0041\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from utils import train_linear_classifier_with_positions\n",
    "\n",
    "\n",
    "def get_probe_results(train_features, train_labels, test_features, test_labels):\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for layer_idx in range(5):           # Layers 0 through 4\n",
    "        results[layer_idx] = {}\n",
    "        \n",
    "        for pos in range(17):           # Positions 0 through 16\n",
    "            run_accuracies = []\n",
    "            \n",
    "            for run_i in range(3):      # Repeat 3 times\n",
    "                # Prepare the position labels (replacing the \"*2\" with \"*pos\")\n",
    "                positions_train = torch.ones(len(train_features)).long() * pos\n",
    "                positions_test  = torch.ones(len(test_features)).long() * pos\n",
    "                \n",
    "                # Train the probe\n",
    "                probe, acc = train_linear_classifier_with_positions(\n",
    "                    train_features,\n",
    "                    positions_train,\n",
    "                    train_labels,\n",
    "                    test_features,\n",
    "                    positions_test,\n",
    "                    test_labels,\n",
    "                    layer_idx=layer_idx,\n",
    "                    num_classes=4,    # or whatever your task requires\n",
    "                    epochs=100,\n",
    "                    batch_size=32,\n",
    "                    device='cuda'\n",
    "                )\n",
    "                \n",
    "                # Extract the accuracy (adjust if your function returns the accuracy differently)\n",
    "                run_accuracies.append(acc)\n",
    "            \n",
    "            # Compute mean and std across the 3 runs\n",
    "            mean_acc = np.mean(run_accuracies)\n",
    "            std_acc  = np.std(run_accuracies)\n",
    "            \n",
    "            # Store in the results structure\n",
    "            results[layer_idx][pos] = (mean_acc, std_acc)\n",
    "\n",
    "    return results\n",
    "\n",
    "small_results = get_probe_results(small_train_features, small_train_labels, small_test_features, small_test_labels)\n",
    "\n",
    "# Now 'results' contains (mean_acc, std_acc) for each (layer_idx, pos).\n",
    "# You can do further processing or just print:\n",
    "for layer_idx in range(5):\n",
    "    for pos in range(17):\n",
    "        mean_acc, std_acc = small_results[layer_idx][pos]\n",
    "        print(f\"Layer {layer_idx}, Pos {pos} -> Mean: {mean_acc:.4f}, Std: {std_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for the large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "large_results = get_probe_results(large_train_features, large_train_labels, large_test_features, large_test_labels)\n",
    "\n",
    "for layer_idx in range(5):\n",
    "    for pos in range(17):\n",
    "        mean_acc, std_acc = large_results[layer_idx][pos]\n",
    "        print(f\"Layer {layer_idx}, Pos {pos} -> Mean: {mean_acc:.4f}, Std: {std_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code to generate the features for the linear probe\n",
    "\n",
    "You need to only load in the relevant model otherwise seems to always OOM? doesn't seem like that should happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "del small_model\n",
    "del large_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For small model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "small_tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "\n",
    "small_filtered_gens = []\n",
    "for item in small_gens:\n",
    "    full_text = item['full_generated_text']\n",
    "    prompt = item['prompt']\n",
    "    # Estimate tokenized length by splitting on whitespace and punctuation\n",
    "    tokenized_length = len(small_tokenizer.tokenize(full_text)) - len(small_tokenizer.tokenize(prompt))\n",
    "\n",
    "    # Check tokenized length condition\n",
    "    if tokenized_length < 2000:\n",
    "        # Extract boxed answer using regex patterns\n",
    "        match = re.search(r'\\\\boxed{\\(?([A-Da-d])\\)?}', full_text)\n",
    "        if match:\n",
    "            label = match.group(1).upper()  # Normalize to uppercase\n",
    "            label_index = ord(label) - ord('A')  # Convert A-D to 0-3 index\n",
    "            answer_str = match.group(0)  # Full match (e.g., \\boxed{C})\n",
    "            char_index = full_text.find(answer_str)  # Character index\n",
    "            token_index = len(small_tokenizer.tokenize(full_text[:char_index]))  # Convert to token index\n",
    "\n",
    "            item['predicted_label'] = label_index\n",
    "            item['answer_token_index'] = token_index  # Store token index of answer\n",
    "            small_filtered_gens.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "Batch 1\n",
      "Batch 2\n",
      "Batch 3\n",
      "Batch 4\n",
      "Batch 5\n",
      "Batch 6\n",
      "Batch 7\n",
      "Batch 8\n",
      "Batch 9\n",
      "Batch 10\n",
      "Batch 11\n",
      "Batch 12\n",
      "Batch 13\n",
      "Batch 14\n",
      "Batch 15\n",
      "Batch 16\n",
      "Batch 17\n",
      "Batch 18\n",
      "Batch 19\n",
      "Batch 20\n",
      "Batch 21\n",
      "Batch 22\n",
      "Batch 23\n",
      "Batch 24\n",
      "Batch 25\n",
      "Batch 26\n",
      "Batch 27\n",
      "Batch 28\n",
      "Batch 29\n",
      "Batch 30\n",
      "Batch 31\n",
      "Batch 32\n",
      "Batch 33\n",
      "Batch 34\n",
      "Batch 35\n",
      "Batch 36\n",
      "Batch 37\n",
      "Batch 38\n",
      "Batch 39\n",
      "Batch 40\n",
      "Batch 41\n",
      "Batch 42\n",
      "Batch 43\n",
      "Batch 44\n",
      "Batch 45\n",
      "Batch 46\n",
      "Batch 47\n",
      "Batch 48\n",
      "Batch 49\n",
      "Batch 50\n",
      "Batch 51\n",
      "Batch 52\n",
      "Batch 53\n",
      "Batch 54\n",
      "Batch 55\n",
      "Batch 56\n",
      "Batch 57\n",
      "Batch 58\n",
      "Batch 59\n",
      "Batch 60\n",
      "Batch 61\n",
      "Batch 62\n",
      "Batch 63\n",
      "Batch 64\n",
      "Batch 65\n",
      "Batch 66\n",
      "Batch 67\n",
      "Batch 68\n",
      "Batch 69\n",
      "Batch 70\n",
      "Batch 71\n",
      "Batch 72\n",
      "Batch 73\n",
      "Batch 74\n",
      "Batch 75\n",
      "Batch 76\n",
      "Batch 77\n",
      "Batch 78\n",
      "Batch 79\n",
      "Batch 80\n",
      "Batch 81\n",
      "Batch 82\n",
      "Batch 83\n",
      "Batch 84\n",
      "Batch 85\n",
      "Batch 86\n",
      "Batch 87\n",
      "Batch 88\n",
      "Batch 89\n",
      "Batch 90\n",
      "Batch 91\n",
      "Batch 92\n",
      "Batch 93\n",
      "Batch 94\n",
      "Batch 95\n",
      "Batch 96\n",
      "Batch 97\n",
      "Batch 98\n",
      "Batch 99\n",
      "Batch 100\n",
      "Batch 101\n",
      "Batch 102\n",
      "Batch 103\n",
      "Batch 104\n",
      "Batch 105\n",
      "Batch 106\n",
      "Batch 107\n",
      "Batch 108\n",
      "Batch 109\n",
      "Batch 110\n",
      "Batch 111\n",
      "Batch 112\n",
      "Batch 113\n",
      "Batch 114\n",
      "Batch 115\n",
      "Batch 116\n",
      "Batch 117\n",
      "Batch 118\n",
      "Batch 119\n",
      "Batch 120\n",
      "Batch 121\n",
      "Batch 122\n",
      "Batch 123\n",
      "Batch 124\n",
      "Batch 125\n",
      "Batch 126\n",
      "Batch 127\n",
      "Batch 128\n",
      "Batch 129\n",
      "Batch 130\n",
      "Batch 131\n",
      "Batch 132\n",
      "Batch 133\n",
      "Batch 134\n",
      "Batch 135\n",
      "Batch 136\n",
      "Batch 137\n",
      "Batch 138\n",
      "Batch 139\n",
      "Batch 140\n",
      "Batch 141\n",
      "Batch 142\n",
      "Batch 143\n",
      "Batch 144\n",
      "Batch 145\n",
      "Batch 146\n",
      "Batch 147\n",
      "Batch 148\n",
      "Batch 149\n",
      "Batch 150\n",
      "Batch 151\n",
      "Batch 152\n",
      "Batch 153\n",
      "Batch 154\n",
      "Batch 155\n",
      "Batch 156\n",
      "Batch 157\n",
      "Batch 158\n",
      "Batch 159\n",
      "Batch 160\n",
      "Batch 161\n",
      "Batch 162\n",
      "Batch 163\n",
      "Batch 164\n",
      "Batch 165\n",
      "Batch 166\n",
      "Batch 167\n",
      "Batch 168\n",
      "Batch 169\n",
      "Batch 170\n",
      "Batch 171\n",
      "Batch 172\n",
      "Batch 173\n",
      "Batch 174\n",
      "Batch 175\n",
      "Batch 176\n",
      "Batch 177\n",
      "Batch 178\n",
      "Batch 179\n",
      "Batch 180\n",
      "Batch 181\n",
      "Batch 182\n",
      "Batch 183\n",
      "Batch 184\n",
      "Batch 185\n",
      "Batch 186\n",
      "Batch 187\n",
      "Batch 188\n",
      "Batch 189\n",
      "Batch 190\n",
      "Batch 191\n",
      "Batch 192\n",
      "Batch 193\n",
      "Batch 194\n",
      "Batch 195\n",
      "Batch 196\n",
      "Batch 197\n",
      "Batch 198\n",
      "Batch 199\n",
      "Batch 200\n",
      "Batch 201\n",
      "Batch 202\n",
      "Batch 203\n",
      "Batch 204\n",
      "Batch 205\n",
      "Batch 206\n",
      "Batch 207\n",
      "Batch 208\n",
      "Batch 209\n",
      "Batch 210\n",
      "Batch 211\n",
      "Batch 212\n",
      "Batch 213\n",
      "Batch 214\n",
      "Batch 215\n",
      "Batch 216\n",
      "Batch 217\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from utils import TextClassificationDataset, get_probe_features\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from nnsight import NNsight\n",
    "import pickle\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "small_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "small_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "small_model = NNsight(small_model)\n",
    "\n",
    "\n",
    "dataset = TextClassificationDataset(small_filtered_gens, small_tokenizer, max_length=2000)\n",
    "dataloader = DataLoader(dataset, batch_size=8)\n",
    "\n",
    "features, labels, _, _, _ = get_probe_features(small_model, dataloader, hidden_dim=1536, layer_indices=[5,10,15,20,25], max_batches=10000)\n",
    "\n",
    "with open('data/probe_features/small_features.pkl', 'wb') as f:\n",
    "\n",
    "    pickle.dump(features, f)\n",
    "\n",
    "with open('data/probe_features/small_labels.pkl', 'wb') as f:\n",
    "\n",
    "    pickle.dump(labels, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large model (batch_size=6 is the max that will not cause an OOM error for me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from utils import TextClassificationDataset, get_probe_features\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "large_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "large_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "large_model = NNsight(large_model)\n",
    "\n",
    "dataset = TextClassificationDataset(large_filtered_gens, small_tokenizer, max_length=2000)\n",
    "dataloader = DataLoader(dataset, batch_size=6)\n",
    "\n",
    "features, labels, _, _, _ = get_probe_features(large_model, dataloader, hidden_dim=3584, layer_indices=[5,10,15,20,25], max_batches=10000)\n",
    "\n",
    "with open('data/probe_features/large_features.pkl', 'wb') as f:\n",
    "\n",
    "    pickle.dump(features, f)\n",
    "\n",
    "with open('data/probe_features/large_labels.pkl', 'wb') as f:\n",
    "\n",
    "    pickle.dump(labels, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnsight_experiments",
   "language": "python",
   "name": "nnsight_experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
