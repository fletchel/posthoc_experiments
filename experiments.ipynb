{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code to replicate the experiments mentioned in the Google doc.\n",
    "\n",
    "The graphs themselves were made by 4o from raw text outputs, so there's no code here for making the actual visualisations.\n",
    "\n",
    "Warning: A lot of this code is ugly and inefficient. Time constraints! If I were doing a larger scale project I would definitely spend some time making the code nicer.\n",
    "\n",
    "Big thanks to my bestie o1 who helped with a lot of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data for below experiments here: https://drive.google.com/drive/folders/18t8bGuHln-PVURLCUX4ny0Us7Q3M5XJ2?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load in the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from nnsight import NNsight\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "small_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "small_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "small_model = NNsight(small_model)\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "large_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "large_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "large_model = NNsight(large_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load in the generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/home/lfletcher/MATS_app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/generations/high_school_gens_large.pkl', 'rb') as f:\n",
    "\n",
    "    large_gens = pickle.load(f)\n",
    "\n",
    "with open('data/generations/high_school_gens_small.pkl', 'rb') as f:\n",
    "\n",
    "    small_gens = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the generations to only consider those <2000 tokens and which follow the format of having an answer in \\boxed{}.\n",
    "\n",
    "This code also adds the predicted_label and token position of the predicted label to the generation dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "small_tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "\n",
    "small_filtered_gens = []\n",
    "for item in small_gens:\n",
    "    full_text = item['full_generated_text']\n",
    "    prompt = item['prompt']\n",
    "    # Estimate tokenized length by splitting on whitespace and punctuation\n",
    "    tokenized_length = len(small_tokenizer.tokenize(full_text)) - len(small_tokenizer.tokenize(prompt))\n",
    "\n",
    "    # Check tokenized length condition\n",
    "    if tokenized_length < 2000:\n",
    "        # Extract boxed answer using regex patterns\n",
    "        match = re.search(r'\\\\boxed{\\(?([A-Da-d])\\)?}', full_text)\n",
    "        if match:\n",
    "            label = match.group(1).upper()  # Normalize to uppercase\n",
    "            label_index = ord(label) - ord('A')  # Convert A-D to 0-3 index\n",
    "            answer_str = match.group(0)  # Full match (e.g., \\boxed{C})\n",
    "            char_index = full_text.find(answer_str)  # Character index\n",
    "            token_index = len(small_tokenizer.tokenize(full_text[:char_index]))  # Convert to token index\n",
    "\n",
    "            item['predicted_label'] = label_index\n",
    "            item['answer_token_index'] = token_index  # Store token index of answer\n",
    "            small_filtered_gens.append(item)\n",
    "\n",
    "large_filtered_gens = []\n",
    "for item in large_gens:\n",
    "    full_text = item['full_generated_text']\n",
    "    prompt = item['prompt']\n",
    "    # Estimate tokenized length by splitting on whitespace and punctuation\n",
    "    tokenized_length = len(small_tokenizer.tokenize(full_text)) - len(small_tokenizer.tokenize(prompt))\n",
    "\n",
    "    # Check tokenized length condition\n",
    "    if tokenized_length < 2000:\n",
    "        # Extract boxed answer using regex patterns\n",
    "        match = re.search(r'\\\\boxed{\\(?([A-Da-d])\\)?}', full_text)\n",
    "        if match:\n",
    "            label = match.group(1).upper()  # Normalize to uppercase\n",
    "            label_index = ord(label) - ord('A')  # Convert A-D to 0-3 index\n",
    "            answer_str = match.group(0)  # Full match (e.g., \\boxed{C})\n",
    "            char_index = full_text.find(answer_str)  # Character index\n",
    "            token_index = len(large_tokenizer.tokenize(full_text[:char_index]))  # Convert to token index\n",
    "\n",
    "            item['predicted_label'] = label_index\n",
    "            item['answer_token_index'] = token_index  # Store token index of answer\n",
    "            large_filtered_gens.append(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we find the incorrect generations for each of the small and large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_incorrect_gens = [gen for gen in small_filtered_gens if gen['predicted_label'] != gen['correct_label']]\n",
    "large_incorrect_gens = [gen for gen in large_filtered_gens if gen['predicted_label'] != gen['correct_label']]\n",
    "                           \n",
    "print(len(small_incorrect_gens))\n",
    "print(len(large_incorrect_gens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the pre and post CoT logits for the predicted_answer for the small and large model\n",
    "\n",
    "This code is monstrously inefficient and not batched. It takes like 10 mins to run on an A100. Sorry!\n",
    "\n",
    "The resulting lists are dictionaries of the form\n",
    "\n",
    "    {\n",
    "        \"final_answer\": the final answer given by the model after the CoT,\n",
    "        \"post_logits\": the logits of A-D after the CoT,\n",
    "        \"pre_logits\": the logits of A-D before the CoT,\n",
    "        \"subject\": the subject of this datapoint (i.e. geography, math etc.)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_pre_post_cot_logits\n",
    "\n",
    "small_logits = []\n",
    "large_logits = []\n",
    "\n",
    "print(\"Getting logits for small model\")\n",
    "for i, gen in enumerate(small_incorrect_gens):\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Getting logits for generation {i}\")\n",
    "\n",
    "    small_logits.append(get_pre_post_cot_logits(gen, small_model, small_tokenizer))\n",
    "\n",
    "print(\"\\nGetting logits for large model\")\n",
    "for i, gen in enumerate(large_incorrect_gens):\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Getting logits for generation {i}\")\n",
    "        \n",
    "    large_logits.append(get_pre_post_cot_logits(gen, large_model, large_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have these logits, we get a couple of relevant quantities\n",
    "\n",
    "1) the probability of the final answer pre and post CoT (saved in cur_prob_dict)\n",
    "2) the probability of all 4 of A-D pre and post CoT (saved in small/large_probs_pre_all and small/large_probs_post_all)\n",
    "\n",
    "The latter we calculate to check some assumptions that we're relying on (see Google doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "small_probs = []\n",
    "\n",
    "small_probs_pre_all = []\n",
    "small_probs_post_all = []\n",
    "\n",
    "for i in range(len(small_incorrect_gens)):\n",
    "\n",
    "    cur_gen = small_logits[i]\n",
    "    predict_idx = small_incorrect_gens[i]['predicted_label']\n",
    "\n",
    "    logits_prompt = cur_gen['pre_logits']\n",
    "    logits_cot = cur_gen['post_logits']\n",
    "    \n",
    "    cur_prob_prompt = F.softmax(logits_prompt)\n",
    "    cur_prob_cot = F.softmax(logits_cot)\n",
    "\n",
    "    small_probs_pre_all.append(cur_prob_prompt.tolist())\n",
    "    small_probs_post_all.append(cur_prob_cot.tolist())\n",
    "\n",
    "    cur_prob_dict = {\"pre_prob\":cur_prob_prompt[predict_idx], \"post_prob\":cur_prob_cot[predict_idx], \"subject\":cur_gen[\"subject\"]}\n",
    "    small_probs.append(cur_prob_dict)\n",
    "\n",
    "\n",
    "large_probs = []\n",
    "\n",
    "large_probs_pre_all = []\n",
    "large_probs_post_all = []\n",
    "\n",
    "for i in range(len(large_incorrect_gens)):\n",
    "\n",
    "    cur_gen = large_logits[i]\n",
    "    predict_idx = large_incorrect_gens[i]['predicted_label']\n",
    "\n",
    "    logits_prompt = cur_gen['pre_logits']\n",
    "    logits_cot = cur_gen['post_logits']\n",
    "    \n",
    "    cur_prob_prompt = F.softmax(logits_prompt)\n",
    "    cur_prob_cot = F.softmax(logits_cot)\n",
    "\n",
    "    large_probs_pre_all.append(cur_prob_prompt.tolist())\n",
    "    large_probs_post_all.append(cur_prob_cot.tolist())\n",
    "\n",
    "    cur_prob_dict = {\"pre_prob\":cur_prob_prompt[predict_idx], \"post_prob\":cur_prob_cot[predict_idx], \"subject\":cur_gen[\"subject\"]}\n",
    "    large_probs.append(cur_prob_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the average probability of the final (post-CoT) answer pre-CoT for each model. This is the main result of this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "subject_stats = defaultdict(lambda: {\"sum\": 0.0, \"count\": 0})\n",
    "\n",
    "# Accumulate sums and counts\n",
    "for entry in small_probs:\n",
    "    subj = entry[\"subject\"]\n",
    "    subject_stats[subj][\"sum\"] += entry[\"pre_prob\"]\n",
    "    subject_stats[subj][\"count\"] += 1\n",
    "\n",
    "# Compute mean and store results\n",
    "result = []\n",
    "for subj, stats in subject_stats.items():\n",
    "    mean_prob = stats[\"sum\"] / stats[\"count\"]\n",
    "    result.append({\n",
    "        \"subject\": subj,\n",
    "        \"mean_pre_prob\": mean_prob,\n",
    "        \"count\": stats[\"count\"]\n",
    "    })\n",
    "\n",
    "print(\"Results for small model\")\n",
    "for entry in result:\n",
    "    print(f\"Subject: {entry['subject']}, Mean Pre-CoT Probability: {entry['mean_pre_prob']:.4f}, Count: {entry['count']}\")\n",
    "\n",
    "\n",
    "# Accumulate sums and counts\n",
    "for entry in large_probs:\n",
    "    subj = entry[\"subject\"]\n",
    "    subject_stats[subj][\"sum\"] += entry[\"pre_prob\"]\n",
    "    subject_stats[subj][\"count\"] += 1\n",
    "\n",
    "# Compute mean and store results\n",
    "result = []\n",
    "for subj, stats in subject_stats.items():\n",
    "    mean_prob = stats[\"sum\"] / stats[\"count\"]\n",
    "    result.append({\n",
    "        \"subject\": subj,\n",
    "        \"mean_pre_prob\": mean_prob,\n",
    "        \"count\": stats[\"count\"]\n",
    "    })\n",
    "\n",
    "print(\"\\nResults for large model\")\n",
    "for entry in result:\n",
    "    print(f\"Subject: {entry['subject']}, Mean Pre-CoT Probability: {entry['mean_pre_prob']:.4f}, Count: {entry['count']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we check for one possible alternative explanation for these results, namely that both the pre and post probabilities are biased in the same way \n",
    "\n",
    "(i.e., if the model says B 50% of the time both pre and post CoT, then our metric of mean pre-CoT probability will be >50% without post-hoc reasoning being necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "small_probs_prompt_all = np.array(small_probs_pre_all)\n",
    "small_probs_cot_all = np.array(small_probs_post_all)\n",
    "\n",
    "large_probs_prompt_all = np.array(large_probs_pre_all)\n",
    "large_probs_cot_all = np.array(large_probs_post_all)\n",
    "\n",
    "print(np.mean(large_probs_prompt_all, axis=0))\n",
    "print(np.mean(large_probs_cot_all, axis=0))\n",
    "\n",
    "print(np.mean(small_probs_prompt_all, axis=0))\n",
    "print(np.mean(small_probs_cot_all, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two distributions are not biased in the same way (although the small model in particular has got a weird bias for B pre-CoT and A post-CoT)\n",
    "\n",
    "In particular, if pre/post were independent, then for both large and small models the probability of ans_pre = ans_post is approximately 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've moved the code which actually extracts the features to the appendix and just load the features here. This is because the code extracting the features is pretty slow (particularly for the big model). Feel free to skip to the appendix to take a look at that if you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we load the features here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/probe_features/small_features.pkl', 'rb') as f:\n",
    "\n",
    "    small_features = pickle.load(f)\n",
    "\n",
    "with open('data/probe_features/small_labels.pkl', 'rb') as f:\n",
    "\n",
    "    small_labels = pickle.load(f)\n",
    "\n",
    "with open('data/probe_features/large_features.pkl', 'rb') as f:\n",
    "\n",
    "    large_features = pickle.load(f)\n",
    "\n",
    "with open('data/probe_features/large_labels.pkl', 'rb') as f:\n",
    "\n",
    "    large_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split into test and train sets, along with randomly discarding datapoints until we have balanced labels [balance_classes()] (this is basically the most naive thing you could possibly do but time constraints!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import balance_classes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepare_splits(features, labels):\n",
    "\n",
    "    bal_features, bal_labels = balance_classes(features, labels)\n",
    "\n",
    "    features_np = features.numpy()\n",
    "    labels_np = labels.numpy()\n",
    "\n",
    "    # Perform an 80–20 split\n",
    "    train_features_np, test_features_np, \\\n",
    "    train_labels_np, test_labels_np = train_test_split(\n",
    "        features_np,\n",
    "        labels_np,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=labels_np  # Stratify for class balance\n",
    "    )\n",
    "\n",
    "    # Convert NumPy arrays back to tensors\n",
    "    train_features = torch.tensor(train_features_np)\n",
    "    test_features  = torch.tensor(test_features_np)\n",
    "    train_labels    = torch.tensor(train_labels_np)\n",
    "    test_labels     = torch.tensor(test_labels_np)\n",
    "\n",
    "    return train_features, test_features, train_labels, test_labels\n",
    "\n",
    "small_train_features, small_test_features, small_train_labels, small_test_labels = prepare_splits(small_features, small_labels)\n",
    "large_train_features, large_test_features, large_train_labels, large_test_labels = prepare_splits(large_features, large_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the linear probe 3 times at each position/layer for the small model and print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from utils import train_linear_classifier_with_positions\n",
    "\n",
    "\n",
    "def get_probe_results(train_features, train_labels, test_features, test_labels):\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for layer_idx in range(5):           # Layers 0 through 4\n",
    "        results[layer_idx] = {}\n",
    "        \n",
    "        for pos in range(17):           # Positions 0 through 16\n",
    "            run_accuracies = []\n",
    "            \n",
    "            for run_i in range(3):      # Repeat 3 times\n",
    "                # Prepare the position labels (replacing the \"*2\" with \"*pos\")\n",
    "                positions_train = torch.ones(len(train_features)).long() * pos\n",
    "                positions_test  = torch.ones(len(test_features)).long() * pos\n",
    "                \n",
    "                # Train the probe\n",
    "                probe, acc = train_linear_classifier_with_positions(\n",
    "                    train_features,\n",
    "                    positions_train,\n",
    "                    train_labels,\n",
    "                    test_features,\n",
    "                    positions_test,\n",
    "                    test_labels,\n",
    "                    layer_idx=layer_idx,\n",
    "                    num_classes=4,    # or whatever your task requires\n",
    "                    epochs=100,\n",
    "                    batch_size=32,\n",
    "                    device='cuda'\n",
    "                )\n",
    "                \n",
    "                # Extract the accuracy (adjust if your function returns the accuracy differently)\n",
    "                run_accuracies.append(acc)\n",
    "            \n",
    "            # Compute mean and std across the 3 runs\n",
    "            mean_acc = np.mean(run_accuracies)\n",
    "            std_acc  = np.std(run_accuracies)\n",
    "            \n",
    "            # Store in the results structure\n",
    "            results[layer_idx][pos] = (mean_acc, std_acc)\n",
    "\n",
    "    return results\n",
    "\n",
    "small_results = get_probe_results(small_train_features, small_train_labels, small_test_features, small_test_labels)\n",
    "\n",
    "# Now 'results' contains (mean_acc, std_acc) for each (layer_idx, pos).\n",
    "# You can do further processing or just print:\n",
    "for layer_idx in range(5):\n",
    "    for pos in range(17):\n",
    "        mean_acc, std_acc = small_results[layer_idx][pos]\n",
    "        print(f\"Layer {layer_idx}, Pos {pos} -> Mean: {mean_acc:.4f}, Std: {std_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for the large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "large_results = get_probe_results(large_train_features, large_train_labels, large_test_features, large_test_labels)\n",
    "\n",
    "for layer_idx in range(5):\n",
    "    for pos in range(17):\n",
    "        mean_acc, std_acc = large_results[layer_idx][pos]\n",
    "        print(f\"Layer {layer_idx}, Pos {pos} -> Mean: {mean_acc:.4f}, Std: {std_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code to generate the features for the linear probe\n",
    "\n",
    "You need to only load in the relevant model otherwise seems to always OOM? doesn't seem like that should happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "del small_model\n",
    "del large_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For small model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "small_tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "\n",
    "small_filtered_gens = []\n",
    "for item in small_gens:\n",
    "    full_text = item['full_generated_text']\n",
    "    prompt = item['prompt']\n",
    "    # Estimate tokenized length by splitting on whitespace and punctuation\n",
    "    tokenized_length = len(small_tokenizer.tokenize(full_text)) - len(small_tokenizer.tokenize(prompt))\n",
    "\n",
    "    # Check tokenized length condition\n",
    "    if tokenized_length < 2000:\n",
    "        # Extract boxed answer using regex patterns\n",
    "        match = re.search(r'\\\\boxed{\\(?([A-Da-d])\\)?}', full_text)\n",
    "        if match:\n",
    "            label = match.group(1).upper()  # Normalize to uppercase\n",
    "            label_index = ord(label) - ord('A')  # Convert A-D to 0-3 index\n",
    "            answer_str = match.group(0)  # Full match (e.g., \\boxed{C})\n",
    "            char_index = full_text.find(answer_str)  # Character index\n",
    "            token_index = len(small_tokenizer.tokenize(full_text[:char_index]))  # Convert to token index\n",
    "\n",
    "            item['predicted_label'] = label_index\n",
    "            item['answer_token_index'] = token_index  # Store token index of answer\n",
    "            small_filtered_gens.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from utils import TextClassificationDataset, get_probe_features\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from nnsight import NNsight\n",
    "import pickle\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "small_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "small_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "small_model = NNsight(small_model)\n",
    "\n",
    "\n",
    "dataset = TextClassificationDataset(small_filtered_gens, small_tokenizer, max_length=2000)\n",
    "dataloader = DataLoader(dataset, batch_size=8)\n",
    "\n",
    "features, labels, _, _, _ = get_probe_features(small_model, dataloader, hidden_dim=1536, layer_indices=[5,10,15,20,25], max_batches=10000)\n",
    "\n",
    "with open('data/probe_features/small_features.pkl', 'wb') as f:\n",
    "\n",
    "    pickle.dump(features, f)\n",
    "\n",
    "with open('data/probe_features/small_labels.pkl', 'wb') as f:\n",
    "\n",
    "    pickle.dump(labels, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large model (batch_size=6 is the max that will not cause an OOM error for me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from utils import TextClassificationDataset, get_probe_features\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "large_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "large_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "large_model = NNsight(large_model)\n",
    "\n",
    "dataset = TextClassificationDataset(large_filtered_gens, small_tokenizer, max_length=2000)\n",
    "dataloader = DataLoader(dataset, batch_size=6)\n",
    "\n",
    "features, labels, _, _, _ = get_probe_features(large_model, dataloader, hidden_dim=3584, layer_indices=[5,10,15,20,25], max_batches=10000)\n",
    "\n",
    "with open('data/probe_features/large_features.pkl', 'wb') as f:\n",
    "\n",
    "    pickle.dump(features, f)\n",
    "\n",
    "with open('data/probe_features/large_labels.pkl', 'wb') as f:\n",
    "\n",
    "    pickle.dump(labels, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnsight_experiments",
   "language": "python",
   "name": "nnsight_experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
